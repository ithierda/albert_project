{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ast\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import re\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPING ANNUAIRE EXPERT COMPTABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_progress_file_path = 'progress_comptable_i3.txt'\n",
    "d_progress_file_path = 'progress_comptable_d3.txt'\n",
    "csv_file_path = 'comptable2_3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_progress(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                return int(line.strip())\n",
    "    return 1  # Démarre de 1 si le fichier n'existe pas ou est vide\n",
    "\n",
    "def save_progress(file_path, progress):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(str(progress))\n",
    "\n",
    "\n",
    "columns = ['company_name', 'forme_juridique', 'telephone', 'nb_exp', 'sites']\n",
    "# creation du fichier csv si il n'existe pas déjà du à une progression antérieure\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "\n",
    "i_cookie = 55#read_progress(i_progress_file_path)\n",
    "# ouvrir l'annuaire des experts comptables\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "# see progression with tqdm starting from the last page\n",
    "max_i = 55\n",
    "# read_progress(i_progress_file_path\n",
    "for i in tqdm(range(55, max_i+1)):\n",
    "    driver.get(f\"https://annuaire.experts-comptables.org/recherche/{i}?seed=33798\")\n",
    "    if i == i_cookie:\n",
    "        cookie_accept = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.XPATH, '//*[@id=\"tarteaucitronPersonalize\"]')))[0]\n",
    "        cookie_accept.click()\n",
    "        driver.maximize_window()\n",
    "    # read_progress(d_progress_file_path)\n",
    "    for d in range(1 , 11):\n",
    "        try:\n",
    "            base_xpath = f\"/html/body/div[1]/div[2]/div/div[2]/div[{d}]\"\n",
    "            societe = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, f\"{base_xpath}/div[2]/div[1]/div/a\"))).text\n",
    "            company_name, forme_juridique = societe.split('\\n')[:2]\n",
    "            try:\n",
    "                telephone_button = WebDriverWait(driver, 1).until(EC.element_to_be_clickable((By.XPATH, f\"{base_xpath}/div[3]/div/div[1]\")))\n",
    "                telephone_button.click()\n",
    "                try:\n",
    "                    telephone = WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.XPATH, '//*[@id=\"mailForm\"]/div/a'))).text\n",
    "                except:\n",
    "                    telephone = np.nan\n",
    "                close_button = WebDriverWait(driver, 5).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"modal-close\"]')))\n",
    "                close_button.click()\n",
    "            except: \n",
    "                telephone = np.nan\n",
    "            try : \n",
    "                savoir_plus_button =  WebDriverWait(driver, 1).until(EC.element_to_be_clickable((By.XPATH, f'{base_xpath}/div[3]/a')))\n",
    "                savoir_plus_button.click()\n",
    "                try :\n",
    "                    nb_exp = len(WebDriverWait(driver, 1).until(EC.presence_of_all_elements_located((By.XPATH, '/html/body/div[1]/div[3]/div[2]/div[2]/div[1]/div/div/div'))))\n",
    "                except : \n",
    "                    nb_exp = np.nan\n",
    "                try :\n",
    "                    element = WebDriverWait(driver, 1).until(EC.visibility_of_element_located((By.XPATH, \"/html/body/div[1]/div[3]/div[2]/div[2]/div[2]/div/p\")))\n",
    "                    sites = element.text\n",
    "                except:\n",
    "                    sites = np.nan\n",
    "                try :\n",
    "                    bouton_retour = WebDriverWait(driver, 1).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/div[1]/div[1]/div/span[2]/a')))\n",
    "                    bouton_retour.click()\n",
    "                except :\n",
    "                    print(f'pas de boutton retour error i: {i}, d:{d}')\n",
    "                    sys.quit(1)\n",
    "            except : \n",
    "                print(f'pas de boutton en savoir plus error i: {i}, d:{d}')\n",
    "                sys.quit(1)                \n",
    "        except:\n",
    "            print(f'societe non trouvé error i: {i}, d:{d}')\n",
    "            continue    \n",
    "\n",
    "\n",
    "        # stockage des données enregistrées\n",
    "        data = {\n",
    "            'company_name': company_name,\n",
    "            'forme_juridique': forme_juridique,\n",
    "            'telephone': telephone,\n",
    "            'nb_exp': nb_exp,\n",
    "            'sites' : sites\n",
    "        }\n",
    "\n",
    "        with open(csv_file_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "            writer.writerow(data)\n",
    "\n",
    "        if d == 10:\n",
    "            save_progress(d_progress_file_path, 1)\n",
    "        else:\n",
    "            save_progress(d_progress_file_path, d)\n",
    "\n",
    "    \n",
    "    save_progress(i_progress_file_path, i)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPING PAPPERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_file_path = 'progress_papers_test.txt'\n",
    "csv_file_path = 'pappers_comptable_test.csv'\n",
    "liste_file_path = 'liste_pappers.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_float(s):\n",
    "    if s == '' or s == np.nan:\n",
    "        return np.nan\n",
    "    if not isinstance(s, str):  # Ensure chaine is a string\n",
    "        return s\n",
    "    # Replace commas with periods to handle European decimal format and remove spaces\n",
    "    s = s.replace(',', '.').replace(' ', '')\n",
    "    multipliers = {'K': 1e3, 'M': 1e6, 'B': 1e9}\n",
    "    # Identify if the last character is a multiplier\n",
    "    last_char = s[-1].upper()  # Use upper to handle lowercase letters\n",
    "    if last_char in multipliers:\n",
    "        # Convert the numeric part to float and apply the multiplier\n",
    "        return float(s[:-1]) * multipliers[last_char]\n",
    "    else:\n",
    "        # If there's no multiplier, directly convert to float\n",
    "        return float(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_progress(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                return int(line.strip()) + 1\n",
    "    return 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(file_path, progress):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(str(progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(liste_file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "liste = content.split('\\n')\n",
    "for i in range(len(liste)):\n",
    "    liste[i] = liste[i].replace(\"'\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"d25b6f1b63f9439193fd13f95a21d97421ba8670669\"\n",
    "proxyModeUrl = \"http://{}:@proxy.scrape.do:8080\".format(token)\n",
    "proxies = {\n",
    "    \"http\": proxyModeUrl,\n",
    "    \"https\": proxyModeUrl,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['nom_beneficiaire','qualite','age','date_de_naissance','company_name', 'siren', 'adresse', 'effectif', 'annee_performance', 'CA', 'resultat_net', 'ebitda', 'papers_link']\n",
    "# creation du fichier csv si il n'existe pas déjà du à une progression antérieure\n",
    "\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "\n",
    "start_from = read_progress(progress_file_path)\n",
    "\n",
    "def session_requests(prox, verify=False):\n",
    "    session = requests.Session()\n",
    "    session.proxies = prox\n",
    "    session.verify = verify\n",
    "    retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "    return session\n",
    "\n",
    "\n",
    "\n",
    "def scrape_data(url, session):\n",
    "    response = session.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        actif_span = soup.find('span', class_='actif')\n",
    "        if actif_span:  # Vérifie si `actif_span` n'est pas None\n",
    "            status_text = actif_span.get_text(strip=True)\n",
    "            if status_text != 'Active':\n",
    "                return\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        \n",
    "        # Siren extraction\n",
    "        siren = url.split('-')[-1] if url else np.nan\n",
    "\n",
    "        # Adresse extraction\n",
    "        adresse_element = soup.find(\"th\", text=\"Adresse :\")\n",
    "        adresse = adresse_element.find_next_sibling(\"td\").text.strip() if adresse_element and adresse_element.find_next_sibling(\"td\") else np.nan\n",
    "\n",
    "        # Effectif extraction\n",
    "        effectif_element = soup.find(\"th\", text=\"Effectif :\")\n",
    "        effectif = effectif_element.find_next_sibling(\"td\").text.strip().split('\\n')[0] if effectif_element and effectif_element.find_next_sibling(\"td\") else np.nan\n",
    "\n",
    "        # Année de performance extraction\n",
    "        annee_performance_element = soup.find(\"th\", text=\"Performance\")\n",
    "        annee_performance = annee_performance_element.find_next_sibling(\"th\").text.strip() if annee_performance_element and annee_performance_element.find_next_sibling(\"th\") else np.nan\n",
    "\n",
    "        # Chiffre d'affaires extraction\n",
    "        ca_element = soup.find(\"th\", text=re.compile(\"Chiffre d'affaires\"))\n",
    "        ca = ca_element.parent.find(\"td\").text.strip() if ca_element and ca_element.parent and ca_element.parent.find(\"td\") else np.nan\n",
    "        ca = convert_to_float(ca) if ca is not np.nan else np.nan\n",
    "\n",
    "        # Résultat net extraction\n",
    "        resultat_net_element = soup.find(\"th\", text=\"Résultat net (€)\")\n",
    "        resultat_net = resultat_net_element.find_next_sibling(\"td\").text.strip() if resultat_net_element and resultat_net_element.find_next_sibling(\"td\") else np.nan\n",
    "        resultat_net = convert_to_float(resultat_net) if resultat_net is not np.nan else np.nan\n",
    "\n",
    "        # EBITDA extraction\n",
    "        ebitda_element = soup.find(\"th\", text=\"EBITDA - EBE (€)\")\n",
    "        ebitda = ebitda_element.find_next_sibling(\"td\").text.strip() if ebitda_element and ebitda_element.find_next_sibling(\"td\") else np.nan\n",
    "        ebitda = convert_to_float(ebitda) if ebitda is not np.nan else np.nan\n",
    "\n",
    "        company_name = ' '.join([part.upper() for part in url.split('/')[-1].split('-') if not part.isdigit() and part])\n",
    "\n",
    "\n",
    "        if ebitda is np.nan and resultat_net is not np.nan:\n",
    "            ebitda = resultat_net * 1.25\n",
    "\n",
    "        for li in soup.select('.beneficiaire'):\n",
    "        # Nom du bénéficiaire\n",
    "            nom_element = li.find('a', class_='nom')\n",
    "            nom = nom_element.text.strip() if nom_element else np.nan\n",
    "            \n",
    "            # Qualité du bénéficiaire\n",
    "            qualite_element = li.find('span', class_='qualite')\n",
    "            qualite = qualite_element.text.strip() if qualite_element else np.nan\n",
    "            \n",
    "            # Âge et date de naissance du bénéficiaire\n",
    "            age_element = li.find('span', class_='age')\n",
    "            if age_element:\n",
    "                age_details = age_element.text.strip().split('-')\n",
    "                age = age_details[0].strip() if age_details else np.nan\n",
    "                date_de_naissance = age_details[-1].strip() if len(age_details) > 1 else np.nan\n",
    "            else:\n",
    "                age = np.nan\n",
    "                date_de_naissance = np.nan\n",
    "\n",
    "            data = {\n",
    "                'nom_beneficiaire': nom,\n",
    "                'qualite': qualite,\n",
    "                'age': age,\n",
    "                'date_de_naissance': date_de_naissance,\n",
    "                'company_name': company_name,  \n",
    "                'siren': siren,\n",
    "                'adresse': adresse,\n",
    "                'effectif': effectif,\n",
    "                'annee_performance': annee_performance,\n",
    "                'CA': ca,\n",
    "                'resultat_net': resultat_net,\n",
    "                'ebitda': ebitda,\n",
    "                'papers_link': url  \n",
    "            }\n",
    "\n",
    "            return data\n",
    "    else:\n",
    "        return \n",
    "\n",
    "def main(liste, start_from, proxies, csv_file_path, progress_file_path, columns):\n",
    "    print(f\"Starting scraping from index {start_from}\")\n",
    "    liste_url = []\n",
    "    for i in range(len(liste)):\n",
    "        url = f\"https://www.pappers.fr/entreprise/{liste[i]}\"\n",
    "        liste_url.append(url)\n",
    "    session = session_requests(proxies)\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = {executor.submit(scrape_data, s, session): s for s in liste_url[start_from:]}\n",
    "        for i, future in enumerate(tqdm(concurrent.futures.as_completed(futures), total=len(liste_url[start_from:]), desc=\"Progression du scraping\")):\n",
    "            s = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    with open(csv_file_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                        writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "                        writer.writerow(result)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred for {s}: {str(e)}\")\n",
    "            finally:\n",
    "                save_progress(progress_file_path, i)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(liste, start_from, proxies, csv_file_path, progress_file_path, columns)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPING LINKEDIN COMPANY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'liste_linkedin_entreprise.txt'\n",
    "csv_file_path = 'entreprise_linkedin.csv'\n",
    "progress_file_path = 'progress_linkedin_entreprise.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_progress(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                return int(line.strip()) + 1\n",
    "    return 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(file_path, progress):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(str(progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "liste = content.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_linkedin = []\n",
    "for i in range(len(liste)):\n",
    "    liste_linkedin.append(liste[i].replace(' ','%20'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_progress(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            line = f.readline()\n",
    "            if line:\n",
    "                return int(line.strip()) + 1\n",
    "    return 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_progress(file_path, progress):\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(str(progress))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [25:13<00:00,  3.37s/it]\n"
     ]
    }
   ],
   "source": [
    "columns = ['company_name','linkedin_company_url', 'site_web', 'taille']\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "# see progression with tqdm starting from the last page\n",
    "driver.get(f\"https://www.linkedin.com/search\")\n",
    "cookie_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"artdeco-global-alert-container\"]/div/section/div/div[2]/button[1]')))\n",
    "cookie_button.click()\n",
    "\n",
    "mail = 'email'\n",
    "password = 'password'  \n",
    "\n",
    "login_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"username\"]')))\n",
    "password_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"password\"]')))\n",
    "connect_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"organic-div\"]/form/div[3]/button')))\n",
    "\n",
    "login_button.send_keys(mail)\n",
    "password_button.send_keys(password)\n",
    "connect_button.click()\n",
    "\n",
    "start_from = read_progress(progress_file_path)\n",
    "\n",
    "for i, company in enumerate(tqdm(liste_linkedin[start_from:]), start=start_from):\n",
    "\n",
    "    driver.get(f\"https://www.linkedin.com/search/results/companies/?companyHqGeo=%5B%22105015875%22%5D&industryCompanyVertical=%5B%2247%22%5D&keywords={company}&origin=FACETED_SEARCH&sid=%3AbD\")\n",
    "    try : \n",
    "        first_link_company_button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.XPATH, '//*[@class=\"search-results-container\"]/div[2]/div/ul/li[1]/div//*[@class=\"display-flex\"]')))\n",
    "        first_link_company_button.click()\n",
    "        try:\n",
    "            a_propos_button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.XPATH, \"//li[contains(@class, 'org-page-navigation__item') and .//text()='À propos']//a\")))\n",
    "            a_propos_button.click()\n",
    "            linkedin_company_url = driver.current_url\n",
    "            a_propos_element = WebDriverWait(driver, 2).until(EC.visibility_of_element_located((By.XPATH, \"//*[@class = 'overflow-hidden']\")))\n",
    "            a_propos = a_propos_element.text.split('\\n')\n",
    "            try :\n",
    "                index = a_propos.index('Site web')\n",
    "                site_web = a_propos[index+1]\n",
    "            except:\n",
    "                site_web = np.nan\n",
    "            try:\n",
    "                index = a_propos.index('Taille de l’entreprise')\n",
    "                taille = a_propos[index+1]\n",
    "            except:\n",
    "                taille = np.nan\n",
    "\n",
    "            data = {'linkedin_company_url':linkedin_company_url,\n",
    "                    'site_web':site_web,\n",
    "                    'taille':taille,\n",
    "                    'company_name': liste[i]}\n",
    "\n",
    "        except:\n",
    "                data = {'linkedin_company_url':linkedin_company_url,\n",
    "                    'site_web':np.nan,\n",
    "                    'taille':np.nan,\n",
    "                    'company_name': liste[i]}\n",
    "    except:\n",
    "        data = {'linkedin_company_url':np.nan,\n",
    "                'site_web':np.nan,\n",
    "                'taille':np.nan,\n",
    "                'company_name': liste[i]}\n",
    "          \n",
    "\n",
    "    with open(csv_file_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                        writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "                        writer.writerow(data)\n",
    "                        \n",
    "    save_progress(progress_file_path, i)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPING LINKEDIN PROFIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'profil_linkedin.csv'\n",
    "progress_file_path = 'progress_linkedin_profil.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'rocket_name_company.csv'\n",
    "rocket_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocket_df['liste'] = rocket_df['Name'] + ', ' + rocket_df['Current Employer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_linkedin = rocket_df['liste'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(liste_linkedin)):\n",
    "    liste_linkedin[i] = liste_linkedin[i].replace(' ','%20').replace(',','%2C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1955/1955 [1:26:03<00:00,  2.64s/it]\n"
     ]
    }
   ],
   "source": [
    "columns = ['Name','company_name','linkedin_profil_url']\n",
    "if not os.path.exists(csv_file_path):\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "# see progression with tqdm starting from the last page\n",
    "driver.get(f\"https://www.linkedin.com/search\")\n",
    "cookie_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"artdeco-global-alert-container\"]/div/section/div/div[2]/button[1]')))\n",
    "cookie_button.click()\n",
    "\n",
    "mail = 'romain_foucault@icloud.com'\n",
    "password = '250768Lebigboss'  \n",
    "\n",
    "login_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"username\"]')))\n",
    "password_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"password\"]')))\n",
    "connect_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"organic-div\"]/form/div[3]/button')))\n",
    "\n",
    "login_button.send_keys(mail)\n",
    "password_button.send_keys(password)\n",
    "connect_button.click()\n",
    "\n",
    "start_from = read_progress(progress_file_path)\n",
    "\n",
    "for i, profil in enumerate(tqdm(liste_linkedin[start_from:]), start=start_from):\n",
    "\n",
    "    driver.get(f\"https://www.linkedin.com/search/results/people/?keywords={profil}&origin=SWITCH_SEARCH_VERTICAL&sid=rxf\")\n",
    "    try : \n",
    "        first_link_profil_button = WebDriverWait(driver, 2).until(EC.element_to_be_clickable((By.XPATH, '(//a[@class=\"app-aware-link \"])[2]')))\n",
    "        linkedin_profil_url = first_link_profil_button.get_attribute('href')\n",
    "    except:\n",
    "        linkedin_profil_url = np.nan\n",
    "\n",
    "    data = {'linkedin_profil_url':linkedin_profil_url,\n",
    "            'Name':rocket_df.loc[i,'Name'],\n",
    "            'company_name': rocket_df.loc[i,'Current Employer']}\n",
    "          \n",
    "\n",
    "    with open(csv_file_path, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "                        writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "                        writer.writerow(data)\n",
    "                        \n",
    "    save_progress(progress_file_path, i)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
